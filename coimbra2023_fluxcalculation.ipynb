{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cffi mode is CFFI_MODE.ANY\n",
      "Unable to determine R home: [WinError 2] Le fichier spécifié est introuvable\n",
      "R home found: C:\\Program Files\\R\\R-4.1.3\n",
      "Default options to initialize R: rpy2, --quiet, --no-save\n",
      "R[write to console]: \n",
      "Attachement du package : 'dplyr'\n",
      "\n",
      "\n",
      "Exception ignored from cffi callback <function _consolewrite_ex at 0x0000020A7FBE0A60>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\phherigcoimb\\Anaconda3\\envs\\talltower\\lib\\site-packages\\rpy2\\rinterface_lib\\callbacks.py\", line 133, in _consolewrite_ex\n",
      "    s = conversion._cchar_to_str_with_maxlen(buf, n, _CCHAR_ENCODING)\n",
      "  File \"c:\\Users\\phherigcoimb\\Anaconda3\\envs\\talltower\\lib\\site-packages\\rpy2\\rinterface_lib\\conversion.py\", line 138, in _cchar_to_str_with_maxlen\n",
      "    s = ffi.string(c, maxlen).decode(encoding)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 30: invalid continuation byte\n",
      "Exception ignored from cffi callback <function _consolewrite_ex at 0x0000020A7FBE0A60>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\phherigcoimb\\Anaconda3\\envs\\talltower\\lib\\site-packages\\rpy2\\rinterface_lib\\callbacks.py\", line 133, in _consolewrite_ex\n",
      "    s = conversion._cchar_to_str_with_maxlen(buf, n, _CCHAR_ENCODING)\n",
      "  File \"c:\\Users\\phherigcoimb\\Anaconda3\\envs\\talltower\\lib\\site-packages\\rpy2\\rinterface_lib\\conversion.py\", line 138, in _cchar_to_str_with_maxlen\n",
      "    s = ffi.string(c, maxlen).decode(encoding)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 30: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "# standard modules\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "# 3rd party modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "pd.DataFrame.columnstartswith = lambda self, x: [c for c in self.columns if c.startswith(x)]\n",
    "pd.DataFrame.columnsmatch = lambda self, x: [c for c in self.columns if re.findall(x, c)]\n",
    "# Project modules\n",
    "import coimbra2023, wavelet_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE\n",
    "SITES_TO_STUDY = ['FR-Fon', 'FR-Gri']\n",
    "\n",
    "# Create setup\n",
    "configure = coimbra2023.structuredData()\n",
    "\n",
    "# Select averaging period\n",
    "configure.averaging = [30, 5]\n",
    "\n",
    "# Select output file path\n",
    "configure.output_path = os.path.join(os.getcwd(), 'data/FR-Gri/CDWT/FR-Gri_CDWT{}_{}.{}mn.csv')\n",
    "\n",
    "# Select raw input file path\n",
    "# e.g.: \"<PROJECT FOLDER>/eddypro_output/eddypro_raw_datasets/level_6/\"\n",
    "configure.raw_kwargs = {'path': os.path.join(os.getcwd(), 'data/FR-Gri/eddypro_output/eddypro_raw_datasets/level_6/')}\n",
    "\n",
    "# Select covariances\n",
    "# x*y → Cov(x, y)\n",
    "# x*y*z*... → Cov(x, y)|Cov(x, z),Cov(x, ...)\n",
    "configure.varstorun = ['w*co2*h2o', 'w*h2o*co2', 'w*ts*co2*h2o']\n",
    "\n",
    "# Select period of interest\n",
    "# [START_DATE, END_DATE, FILE_FREQUENCY]\n",
    "configure.ymd = ['202201010030', '202201020000', '30min']\n",
    "\n",
    "# Select wavelet method\n",
    "configure.method = 'dwt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flux processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN WAVELET FLUX PROCESSING\n",
    "rdw = wavelet_functions.run_wt(**vars(configure), verbosity=5)\n",
    "\n",
    "# Merge into a single file\n",
    "coimbra2023.concat_into_single_file(\n",
    "    os.path.dirname(configure.output_path), \n",
    "    'FR-Gri_CDWT_full_cospectra.+.30mn.csv', \n",
    "    output_path=os.path.join('data/FR-Gri/FR-Gri_CDWT_full_cospectra_unique.30mn.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN GAP FILLING\n",
    "for sitename in SITES_TO_STUDY:\n",
    "    data = coimbra2023.get_eddypro_output(sitename)[['TIMESTAMP', 'flag(w)', 'flag(w/co2)', 'co2_flux', \n",
    "                                                  'latitude', 'longitude', 'air_temperature', 'RH', 'u*', 'Vd']]\n",
    "\n",
    "    data = data.merge(\n",
    "        coimbra2023.get_biomet(sitename)[['TIMESTAMP', 'SW_IN']], on='TIMESTAMP', how='outer')\n",
    "\n",
    "    previous_columns = data.columns\n",
    "    data = data.merge(\n",
    "        coimbra2023.get_cospectra(sitename), on='TIMESTAMP', how='left')\n",
    "\n",
    "    for c in set(data.columns) - set(previous_columns):\n",
    "        data[c] = data[c] / data.Vd\n",
    "\n",
    "    wco2_cols = data.columnsmatch('^wco2(..wh2o)$')\n",
    "\n",
    "    gapf_data = coimbra2023.gap_filling(data, latitude=data.latitude[0], longitude=data.longitude[0], \n",
    "        cols={'DW': {'var': 'wco2', 'flag': {'flag(w)': 2}, 'flux': wco2_cols},\n",
    "              'EC': {'var': 'co2_flux', 'flag': {'flag(w)': 2, 'flag(w/co2)': 2}}})\n",
    "    gapf_data.to_csv(f'data/{sitename}/{sitename}_flux_MDS_gapfilled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN PARTITIONING\n",
    "for sitename in SITES_TO_STUDY:\n",
    "    data = coimbra2023.get_eddypro_output(sitename)[['TIMESTAMP', 'flag(w)', 'flag(w/co2)', 'co2_flux', \n",
    "                                                  'latitude', 'longitude', 'air_temperature', 'RH', 'u*', 'Vd']]\n",
    "\n",
    "    data = data.merge(\n",
    "        coimbra2023.get_biomet(sitename)[['TIMESTAMP', 'SW_IN']], on='TIMESTAMP', how='outer')\n",
    "    \n",
    "    coimbra2023.mkdirs('data/tmp/')\n",
    "    data.to_csv(f'data/tmp/data_to_be_partitioned_{sitename}.csv', index=False)\n",
    "\n",
    "    coimbra2023.routine_partition(sitename, os.getcwd()+'data', \n",
    "                                  os.getcwd()+f'data/tmp/data_to_be_partitioned_{sitename}.csv', \n",
    "                                  ec_name=\"co2_flux\", dw_name=\"wco2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consolidate dataset as one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flux_data = coimbra2023.get_eddypro_output()\n",
    "\n",
    "all_flux_data = all_flux_data.merge(\n",
    "    coimbra2023.get_biomet(), on=['co_site', 'TIMESTAMP'], how='outer', suffixes=('', '_BM'))\n",
    "\n",
    "#all_flux_data = all_flux_data.merge(\n",
    "#    coimbra2023.get_metadata(), on='co_site', how='outer', suffixes=('', '_META'))\n",
    "\n",
    "previous_columns = all_flux_data.columns\n",
    "all_flux_data = all_flux_data.merge(\n",
    "    coimbra2023.get_cospectra(), on=['co_site', 'TIMESTAMP'], how='left', suffixes=('', '_DWCS'))\n",
    "\n",
    "new_columns = set(all_flux_data.columns) - set(previous_columns)\n",
    "for c in new_columns:\n",
    "    all_flux_data[c] = all_flux_data[c] / all_flux_data.Vd\n",
    "    all_flux_data.rename(columns={c: 'DW_' + c}, inplace=True)\n",
    "del new_columns, previous_columns\n",
    "\n",
    "all_flux_data = all_flux_data.merge(\n",
    "    coimbra2023.get_gapfilling(), on=['co_site', 'TIMESTAMP'], how='left', suffixes=('', '_GAPF'))\n",
    "\n",
    "all_flux_data = all_flux_data.merge(\n",
    "    coimbra2023.get_partitioning(), on=['co_site', 'TIMESTAMP'], how='outer', suffixes=('', '_PT'))\n",
    "\n",
    "# day or night\n",
    "all_flux_data['daynight'] = (all_flux_data.TIMESTAMP.dt.hour > 9) * (all_flux_data.TIMESTAMP.dt.hour < 16)\n",
    "all_flux_data.loc[np.isnan(all_flux_data.PPFD_IN)==False, 'daynight'] = (all_flux_data.daynight==False * (all_flux_data.PPFD_IN < 10))==False  \n",
    "\n",
    "# apply partitioning\n",
    "all_flux_data = coimbra2023.partitionDWCS(all_flux_data, GPP='DW_GPP', Reco='DW_Reco', NEE='DW_NEE_uStar_f', \n",
    "                                          positive='DW_wco2-+wh2o_uStar_f', negative='DW_wco2--wh2o_uStar_f')\n",
    "\n",
    "get_dic_flux_data = lambda data=all_flux_data: {k: copy.deepcopy(data.query(f\"co_site == '{k}'\").reset_index(drop=True)) for k in data.co_site.unique()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talltower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
